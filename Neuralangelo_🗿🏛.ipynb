{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangjiajiTHU/BlenderNeuralangelo/blob/main/Neuralangelo_%F0%9F%97%BF%F0%9F%8F%9B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K8vzwXczDPt"
      },
      "source": [
        "# Neuralangelo (Colab demo)\n",
        "This is a Google Colab example for running Neuralangelo.  \n",
        "\n",
        "<img src=\"https://github.com/NVlabs/neuralangelo/raw/main/assets/teaser.gif\">\n",
        "\n",
        "**Neuralangelo: High-Fidelity Neural Surface Reconstruction**  \n",
        "[Zhaoshuo Li](https://mli0603.github.io/),\n",
        "[Thomas Müller](https://tom94.net/),\n",
        "[Alex Evans](https://research.nvidia.com/person/alex-evans),\n",
        "[Russell H. Taylor](https://www.cs.jhu.edu/~rht/),\n",
        "[Mathias Unberath](https://mathiasunberath.github.io/),\n",
        "[Ming-Yu Liu](https://mingyuliu.net/),\n",
        "[Chen-Hsuan Lin](https://chenhsuanlin.bitbucket.io/)  \n",
        "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023\n",
        "\n",
        "[Project page](https://research.nvidia.com/labs/dir/neuralangelo/) | [Paper](https://arxiv.org/abs/2306.03092/)\n",
        "\n",
        "### 💻 Get started with the full code! --> [Github repo](https://github.com/nvlabs/neuralangelo)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPUEBkhiCkic"
      },
      "source": [
        "### Notes\n",
        "- This is a preview of how Neuralangelo works. For full reproduction of results, please check out our [Github repo](https://github.com/nvlabs/neuralangelo).  \n",
        "- Please make sure to connect to a runtime session with a GPU device."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5_BfIKn7mqb"
      },
      "source": [
        "First, clone the Neuralangelo repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OimV7r3Xx_fR"
      },
      "outputs": [],
      "source": [
        "# @title { vertical-output: true }\n",
        "!git clone https://github.com/nvlabs/neuralangelo\n",
        "%cd /content/neuralangelo\n",
        "!git submodule update --init --recursive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9BK79ML7qm-"
      },
      "source": [
        "Install COLMAP (takes ~2 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahZzbkIbSQAL"
      },
      "outputs": [],
      "source": [
        "# @title { vertical-output: true }\n",
        "%cd /content\n",
        "!apt-get install \\\n",
        "    ninja-build \\\n",
        "    build-essential \\\n",
        "    libboost-program-options-dev \\\n",
        "    libboost-filesystem-dev \\\n",
        "    libboost-graph-dev \\\n",
        "    libboost-system-dev \\\n",
        "    libeigen3-dev \\\n",
        "    libflann-dev \\\n",
        "    libfreeimage-dev \\\n",
        "    libmetis-dev \\\n",
        "    libgoogle-glog-dev \\\n",
        "    libgtest-dev \\\n",
        "    libsqlite3-dev \\\n",
        "    libglew-dev \\\n",
        "    qtbase5-dev \\\n",
        "    libqt5opengl5-dev \\\n",
        "    libcgal-dev \\\n",
        "    libceres-dev\n",
        "!apt-get install xvfb\n",
        "# libglvnd is needed for COLMAP to run on Google Colab (https://github.com/colmap/colmap/issues/1271#issuecomment-931900582)\n",
        "!git clone https://github.com/NVIDIA/libglvnd\n",
        "%cd /content/libglvnd\n",
        "!apt-get install libxext-dev libx11-dev x11proto-gl-dev\n",
        "!apt-get install autoconf automake libtool\n",
        "!apt-get install libffi-dev\n",
        "!./autogen.sh\n",
        "!./configure\n",
        "!make -j4\n",
        "!make install\n",
        "# Download and extract the pre-compiled COLMAP library\n",
        "%cd /content\n",
        "!gdown 1PyyNKY2mt4dHlYN5WcPnvWdV2nufQru1\n",
        "!tar -C /usr -zxf colmap-3.8.tar.gz\n",
        "# Install other Python libraries for the data preparation scripts\n",
        "!pip install \\\n",
        "    addict \\\n",
        "    k3d \\\n",
        "    opencv-python-headless \\\n",
        "    pillow \\\n",
        "    plotly \\\n",
        "    pyyaml \\\n",
        "    trimesh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISiHhYVs7W9e"
      },
      "source": [
        "Download the Lego toy example video and visualize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUhJIThkeQoi"
      },
      "outputs": [],
      "source": [
        "# @title { vertical-output: true }\n",
        "%cd /content/neuralangelo\n",
        "# Download a toy example video. The video will be saved as lego.mp4\n",
        "!gdown 1yWoZ4Hk3FgmV3pd34ZbW7jEqgqyJgzHy\n",
        "# Take a look at the video.\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open(\"lego.mp4\", \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video src=\"{data_url}\" width=400 controls></video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT3-T-ABLozw"
      },
      "source": [
        "Preprocess the Lego toy example video (end-to-end version, including COLMAP).\n",
        "- There are 200 frames in the video. We set `DOWNSAMPLE_RATE=2` to downsample the video and extract 100 frames.\n",
        "- We set the scene type to `object` (simple object, not modeling varying appearances).\n",
        "- In most cases, COLMAP should be able to register all the 100 images. In the rare case that it doesn't:\n",
        "  - Try rerunning it. Structure from motion in COLMAP is not fully deterministic.\n",
        "  - Neuralangelo can still run with only a subset of registered images, but the quality may degrade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKhmhsJMLia-"
      },
      "outputs": [],
      "source": [
        "# @title { vertical-output: true }\n",
        "%cd /content/neuralangelo\n",
        "# Set variables.\n",
        "SEQUENCE = \"lego\"\n",
        "PATH_TO_VIDEO = \"lego.mp4\"\n",
        "DOWNSAMPLE_RATE = 2\n",
        "SCENE_TYPE = \"object\"  # {outdoor,indoor,object}\n",
        "# Run the script.\n",
        "colmap_path = f\"datasets/{SEQUENCE}_ds{DOWNSAMPLE_RATE}\"\n",
        "!rm -rf {colmap_path}\n",
        "!bash projects/neuralangelo/scripts/preprocess.sh {SEQUENCE} {PATH_TO_VIDEO} {DOWNSAMPLE_RATE} {SCENE_TYPE}\n",
        "# Check whether we have 100 images registered.\n",
        "import os\n",
        "num_images = len(os.listdir(f\"{colmap_path}/images\"))\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"Number of registered images: {num_images}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXmNCbAa7ucr"
      },
      "source": [
        "Let's inspect the COLMAP results. First, we load the COLMAP data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIanVCvsXkij"
      },
      "outputs": [],
      "source": [
        "# @title { vertical-output: true }\n",
        "%cd /content/neuralangelo\n",
        "# Import Python libraries.\n",
        "import numpy as np\n",
        "import torch\n",
        "import k3d\n",
        "import json\n",
        "import plotly.graph_objs as go\n",
        "from collections import OrderedDict\n",
        "# Import imaginaire modules.\n",
        "from projects.nerf.utils import camera, visualize\n",
        "from third_party.colmap.scripts.python.read_write_model import read_model\n",
        "# Read the COLMAP data.\n",
        "cameras, images, points_3D = read_model(path=f\"{colmap_path}/sparse\", ext=\".bin\")\n",
        "# Convert camera poses.\n",
        "images = OrderedDict(sorted(images.items()))\n",
        "qvecs = torch.from_numpy(np.stack([image.qvec for image in images.values()]))\n",
        "tvecs = torch.from_numpy(np.stack([image.tvec for image in images.values()]))\n",
        "Rs = camera.quaternion.q_to_R(qvecs)\n",
        "poses = torch.cat([Rs, tvecs[..., None]], dim=-1)  # [N,3,4]\n",
        "print(f\"# images: {len(poses)}\")\n",
        "# Get the sparse 3D points and the colors.\n",
        "xyzs = torch.from_numpy(np.stack([point.xyz for point in points_3D.values()]))\n",
        "rgbs = np.stack([point.rgb for point in points_3D.values()])\n",
        "rgbs_int32 = (rgbs[:, 0] * 2**16 + rgbs[:, 1] * 2**8 + rgbs[:, 2]).astype(np.uint32)\n",
        "print(f\"# points: {len(xyzs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eewWCIIeRedG"
      },
      "source": [
        "This is where you should visualize and adjust the bounding sphere for Neuralangelo.\n",
        "- Use the forms to tune `readjust_center` and `readjust_scale` to adjust the bounding sphere.\n",
        "  - The bounding sphere should ideally *just* encapsulate the target object/scene.\n",
        "  - In the Lego toy example case, setting `readjust_scale=0.5` would be a good choice.\n",
        "- Also check whether the camera trajectory matches the expectation from the video observation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfKF43Iy0zNz"
      },
      "outputs": [],
      "source": [
        "# @title { vertical-output: true }\n",
        "# Visualize the bounding sphere.\n",
        "json_fname = f\"{colmap_path}/transforms.json\"\n",
        "with open(json_fname) as file:\n",
        "    meta = json.load(file)\n",
        "center = meta[\"sphere_center\"]\n",
        "radius = meta[\"sphere_radius\"]\n",
        "# ------------------------------------------------------------------------------------\n",
        "# These variables can be adjusted to make the bounding sphere fit the region of interest.\n",
        "# The adjusted values can then be set in the config as data.readjust.center and data.readjust.scale\n",
        "readjust_x = 0.  # @param {type:\"number\"}\n",
        "readjust_y = 0.  # @param {type:\"number\"}\n",
        "readjust_z = 0.  # @param {type:\"number\"}\n",
        "readjust_scale = 1.  # @param {type:\"number\"}\n",
        "readjust_center = np.array([readjust_x, readjust_y, readjust_z])\n",
        "# ------------------------------------------------------------------------------------\n",
        "center += readjust_center\n",
        "radius *= readjust_scale\n",
        "# Make some points to hallucinate a bounding sphere.\n",
        "sphere_points = np.random.randn(100000, 3)\n",
        "sphere_points = sphere_points / np.linalg.norm(sphere_points, axis=-1, keepdims=True)\n",
        "sphere_points = sphere_points * radius + center"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEOaONsbSMg2"
      },
      "source": [
        "Visualize the bounding sphere in the 3D interactive visualizer.\n",
        "- If the bounding sphere doesn't look right, readjust in the above form and rerun the code block.\n",
        "- You can modify `vis_depth` to adjust the size of the cameras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMhKrNV4Y9DE"
      },
      "outputs": [],
      "source": [
        "# @title { vertical-output: true }\n",
        "vis_depth = 0.2\n",
        "# Visualize with Plotly.\n",
        "x, y, z = *xyzs.T,\n",
        "colors = rgbs / 255.0\n",
        "sphere_x, sphere_y, sphere_z = *sphere_points.T,\n",
        "sphere_colors = [\"#4488ff\"] * len(sphere_points)\n",
        "traces_poses = visualize.plotly_visualize_pose(poses, vis_depth=vis_depth, xyz_length=0.02, center_size=0.01, xyz_width=0.005, mesh_opacity=0.05)\n",
        "trace_points = go.Scatter3d(x=x, y=y, z=z, mode=\"markers\", marker=dict(size=1, color=colors, opacity=1), hoverinfo=\"skip\")\n",
        "trace_sphere = go.Scatter3d(x=sphere_x, y=sphere_y, z=sphere_z, mode=\"markers\", marker=dict(size=0.5, color=sphere_colors, opacity=0.7), hoverinfo=\"skip\")\n",
        "traces_all = traces_poses + [trace_points, trace_sphere]\n",
        "layout = go.Layout(scene=dict(xaxis=dict(showspikes=False, backgroundcolor=\"rgba(0,0,0,0)\", gridcolor=\"rgba(0,0,0,0.1)\"),\n",
        "                              yaxis=dict(showspikes=False, backgroundcolor=\"rgba(0,0,0,0)\", gridcolor=\"rgba(0,0,0,0.1)\"),\n",
        "                              zaxis=dict(showspikes=False, backgroundcolor=\"rgba(0,0,0,0)\", gridcolor=\"rgba(0,0,0,0.1)\"),\n",
        "                              xaxis_title=\"X\", yaxis_title=\"Y\", zaxis_title=\"Z\", dragmode=\"orbit\",\n",
        "                              aspectratio=dict(x=1, y=1, z=1), aspectmode=\"data\"), height=800)\n",
        "fig = go.Figure(data=traces_all, layout=layout)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6_ZFujL70MF"
      },
      "source": [
        "Now we are ready to run Neuralangelo. First, we install PyTorch and other libraries (takes ~1 minute).\n",
        "- Ignore the warning about restarting the runtime for `ipywidgets`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MItsTPFgMcqV"
      },
      "outputs": [],
      "source": [
        "# @title { vertical-output: true }\n",
        "%cd /content/neuralangelo\n",
        "# Install PyTorch.\n",
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "# Install Neuralangelo dependencies (excluding tiny-cuda-nn).\n",
        "with open(\"requirements.txt\") as file, open(\"requirements1.txt\", \"w\") as file1:\n",
        "    for line in file:\n",
        "        if \"tiny-cuda-nn\" in line:\n",
        "            continue\n",
        "        file1.write(line)\n",
        "!pip install -r requirements1.txt\n",
        "# Download and extract the pre-compiled tiny-cuda-nn.\n",
        "%cd /content\n",
        "!gdown 1Ah-SKJufHE6BmjF96ic-zUjZ_ic90JZ5\n",
        "!pip install tinycudann-1.7-cp310-cp310-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rM8A4Y_74l8"
      },
      "source": [
        "Let's run Neuralangelo! We use a simplified setup by adjusting the following hyperparameters:\n",
        "- `max_iter` to 20k optimization steps.\n",
        "- Disabling validation steps (by setting `validation_iter` to a large number).\n",
        "- Smaller `model.object.sdf.encoding.coarse2fine.step` to add progressive levels faster.\n",
        "- Smaller `model.object.sdf.encoding.hashgrid.dict_size` since the object is relatively less complex.\n",
        "- Setting `data.readjust.scale=0.5` as suggested above.\n",
        "\n",
        "Neuralangelo under this setup takes ~2 hours on a T4 GPU.  \n",
        "(Maybe grab a ☕️ or 🧋 while we wait!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-18e9hXv2-4"
      },
      "outputs": [],
      "source": [
        "# @title { vertical-output: true }\n",
        "%cd /content/neuralangelo\n",
        "GROUP = \"test_exp\"\n",
        "NAME = \"lego\"\n",
        "!torchrun --nproc_per_node=1 train.py \\\n",
        "    --logdir=logs/{GROUP}/{NAME} \\\n",
        "    --show_pbar \\\n",
        "    --config=projects/neuralangelo/configs/custom/lego.yaml \\\n",
        "    --data.readjust.scale=0.5 \\\n",
        "    --max_iter=20000 \\\n",
        "    --validation_iter=99999999 \\\n",
        "    --model.object.sdf.encoding.coarse2fine.step=200 \\\n",
        "    --model.object.sdf.encoding.hashgrid.dict_size=19 \\\n",
        "    --optim.sched.warm_up_end=200 \\\n",
        "    --optim.sched.two_steps=[12000,16000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkqNxD5R77gc"
      },
      "source": [
        "Finally, let's extract the 3D mesh!\n",
        "- We default `--resolution` (3D mesh resolution) to 300 due to the constraints of Colab.\n",
        "- Add `--textured` to extract the colors as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABAQV2EjjJN3"
      },
      "outputs": [],
      "source": [
        "# @title { vertical-output: true }\n",
        "%cd /content/neuralangelo\n",
        "mesh_fname = f\"logs/{GROUP}/{NAME}/mesh.ply\"\n",
        "!torchrun --nproc_per_node=1 projects/neuralangelo/scripts/extract_mesh.py \\\n",
        "    --config=logs/{GROUP}/{NAME}/config.yaml \\\n",
        "    --checkpoint=logs/{GROUP}/{NAME}/epoch_00400_iteration_000020000_checkpoint.pt \\\n",
        "    --output_file={mesh_fname} \\\n",
        "    --resolution=300 --block_res=128 \\\n",
        "    --textured"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzSkaBHW7-46"
      },
      "source": [
        "Visualize our final textured 3D mesh! 🤩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbx2w7KlnXPK"
      },
      "outputs": [],
      "source": [
        "# @title { vertical-output: true }\n",
        "import numpy as np\n",
        "import trimesh\n",
        "# Load the mesh.\n",
        "mesh = trimesh.load(mesh_fname)\n",
        "print(f\"# vertices: {len(mesh.vertices)}\")\n",
        "print(f\"# faces: {len(mesh.faces)}\")\n",
        "# Create a Trimesh scene and visualize the mesh.\n",
        "scene = trimesh.Scene()\n",
        "scene.add_geometry(mesh)\n",
        "scene.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some final notes (on the resulting bulldozer model)\n",
        "- It looks darker due to insufficient lighting in the 3D visualizer.\n",
        "- The quality of the reconstructed geometry can be further improved by\n",
        "    - training longer\n",
        "    - training with more GPUs\n",
        "    - extracting the mesh with higher resolution\n",
        "- There are random white blobs around the bulldozer. This is normal because the geometry is ambiguous in the white background regions (without other regularizations). It is typically not an issue for real-world video captures.\n",
        "\n",
        "### 💻 Get started with the full code! --> [Github repo](https://github.com/nvlabs/neuralangelo)"
      ],
      "metadata": {
        "id": "F8STJ9WEfpZq"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}